{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Usecase2_reviews_posted_by_reviewers_working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnx6MoDzRJkJ"
      },
      "source": [
        "### 0. Pip Install & Load Relevant Libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ytJnqNRKEA",
        "outputId": "ba47f90e-0708-45ca-e6dc-bc1bff5ed9d7"
      },
      "source": [
        "# To mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Warnings\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "\n",
        "# To quickly access files\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# html\n",
        "import html\n",
        "\n",
        "# Normalized from longform UTF-8 to ASCII encoding. This will remove accents in characters and ensure that words like \"naïve\" will simply be interpreted as (and therefore not differentiated from) \"naive\".\n",
        "from unicodedata import normalize\n",
        "\n",
        "# import nltk resources\n",
        "import nltk\n",
        "\n",
        "resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
        "             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\",\"wordnet\"]\n",
        "\n",
        "for resource in resources:\n",
        "    try:\n",
        "        nltk.data.find(\"tokenizers/\" + resource)\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "        \n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "# string library\n",
        "import string\n",
        "\n",
        "# regex\n",
        "import re\n",
        "\n",
        "# matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# time \n",
        "import time\n",
        "\n",
        "# numpy\n",
        "import numpy as np\n",
        "\n",
        "# progress bar\n",
        "try:\n",
        "    from progress.bar import Bar\n",
        "except:\n",
        "    !pip install progress\n",
        "    from progress.bar import Bar\n",
        "\n",
        "# import ast\n",
        "import ast\n",
        "\n",
        "import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8FeH5tXRM8z"
      },
      "source": [
        "### 1. Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKyom-KiRNXM",
        "outputId": "cbc40bc5-110d-43f2-e57e-8c11b3d806f5"
      },
      "source": [
        "# Mount drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRis-jQqCwnt"
      },
      "source": [
        "### 2. Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvX2nSrCyAM"
      },
      "source": [
        "def cd_drive(mode):\n",
        "    if mode == 'home':\n",
        "        %cd '/content/drive/My Drive/FYP/FYP Y4S1/usecase2'\n",
        "    elif mode == 'base':\n",
        "        %cd '/content/drive/My Drive/FYP/FYP Y4S1/review_activity/base'\n",
        "    elif mode == 'interim':\n",
        "        %cd '/content/drive/My Drive/FYP/FYP Y4S1/review_activity/interim'\n",
        "    elif mode == 'processed':\n",
        "        %cd '/content/drive/My Drive/FYP/FYP Y4S1/review_activity/processed'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRM-YLTLAy-I"
      },
      "source": [
        "### 3. Load base Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY7RaY6dA2Wt",
        "outputId": "cc5ff119-8254-4462-992e-4de38416f312"
      },
      "source": [
        "cd_drive('interim')\r\n",
        "# profiles_df = pd.read_csv('consolidated_profiles_feature_engineering.csv',encoding='utf-8')\r\n",
        "# display(profiles_df)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bpW1tlGnHpN_Bnj5vXSvvN4WcvlMjcHm/FYP Y4S1/review_activity/interim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFFG5n6tVbLV"
      },
      "source": [
        "### 4. Seperate reviews in reviewer contribution column into dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT3HS8IDXXMX"
      },
      "source": [
        "# def extract_review_activity(df):\n",
        "#     cols=['review_id','acc_num','asin','sortTimestamp','rating','helpfulVotes','reviewCount','title','text','images_posted','verifiedPurchase']\n",
        "    \n",
        "#     extracted_data = []\n",
        "#     for index, row in df.iterrows():\n",
        "#         print(\"Extracting {} out of {}...\".format(index+1,len(df)))\n",
        "#         not_empty_data = True\n",
        "#         try:\n",
        "#             contribution_data = ast.literal_eval(row['reviewer_contributions'])\n",
        "#             if contribution_data == []:\n",
        "#                 not_empty_data = False\n",
        "#         except:\n",
        "#             not_empty_data = False\n",
        "#         if not_empty_data:\n",
        "#             count = 0\n",
        "#             for data in contribution_data:\n",
        "#                 data = dict(data)\n",
        "#                 if 'ideas' not in data['id']:\n",
        "#                     try:\n",
        "#                         acc_num = row['acc_num']\n",
        "#                         review_id = data['externalId']\n",
        "#                         sortTimestamp = data['sortTimestamp']\n",
        "#                         text = data['text']\n",
        "#                         asin = data['product']['asin']\n",
        "#                         verifiedPurchase = data['verifiedPurchase']\n",
        "\n",
        "#                         if 'rating' not in data:\n",
        "#                             rating = None\n",
        "#                         else:\n",
        "#                             rating = data['rating']\n",
        "#                         if 'helpfulVotes' not in data:\n",
        "#                             helpfulVotes = None\n",
        "#                         else:\n",
        "#                             helpfulVotes = data['helpfulVotes']\n",
        "#                         if 'reviewCount' not in data:\n",
        "#                             reviewCount = None\n",
        "#                         else:\n",
        "#                             reviewCount = data['reviewCount']\n",
        "#                         if 'title' not in data:\n",
        "#                             title = ''\n",
        "#                         else:\n",
        "#                             title = data['title']\n",
        "#                         if 'images' not in data:\n",
        "#                             images_posted = None\n",
        "#                         else:\n",
        "#                             images_posted = len(data['images'])\n",
        "\n",
        "#                         extracted_data.append([review_id,acc_num,asin,sortTimestamp,rating,helpfulVotes,reviewCount,title,text,images_posted,verifiedPurchase])\n",
        "#                         count += 1\n",
        "#                         print(\"Added {} out of {} from reviewer contribution...\".format(count,len(data)))\n",
        "#                     except:\n",
        "#                         continue\n",
        "\n",
        "#             print(\"Reviewer Activity Dataset currently has {} rows...\\n\".format(len(extracted_data)))\n",
        "\n",
        "#     review_activity_df = pd.DataFrame(extracted_data,columns=cols)\n",
        "    \n",
        "#     print(\"Starting preprocessing on Reviewer Activity Dataset soon..\")\n",
        "#     return review_activity_df.drop_duplicates(subset='review_id')\n",
        "\n",
        "# review_activity_df = extract_review_activity(profiles_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnxCOkM8pq9b"
      },
      "source": [
        "### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-XhkXXWptSF"
      },
      "source": [
        "# review_activity_df = pd.read_csv('consolidated_review_activity_feature_engineering.csv',encoding='utf-8')\r\n",
        "# display(review_activity_df)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkNZAHZ-2Jb1"
      },
      "source": [
        "### Single Day Reviewer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1j63LeD2Lk4"
      },
      "source": [
        "# review_activity_df['cleaned_datetime_posted'] = pd.to_datetime(review_activity_df['cleaned_datetime_posted'], errors='coerce')\r\n",
        "# datetime_review_activity_df = review_activity_df.groupby([review_activity_df['cleaned_datetime_posted'].dt.date,'acc_num']).size().reset_index(name='count')\r\n",
        "# single_day_reviewers = list(set(datetime_review_activity_df[datetime_review_activity_df['count'] > 1]['acc_num']))\r\n",
        "# profiles_df['cleaned_single_day_reviewer'] = 0\r\n",
        "\r\n",
        "# profiles_df.loc[profiles_df['acc_num'].isin(single_day_reviewers), 'cleaned_single_day_reviewer'] = 1\r\n",
        "# display(profiles_df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3EOrgCC0l-v"
      },
      "source": [
        "### Total Products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZU4ns_p0nzL"
      },
      "source": [
        "# total_products_df = review_activity_df.groupby(['acc_num']).agg({\"asin\":\"nunique\"}).reset_index()\r\n",
        "# total_products_df = total_products_df.rename(columns={\"asin\":\"cleaned_total_product\"})\r\n",
        "\r\n",
        "# loreal_products_df = review_activity_df[review_activity_df['cleaned_loreal_review'] == 1].groupby(['acc_num']).agg({\"asin\":\"nunique\"}).reset_index()\r\n",
        "# loreal_products_df = loreal_products_df.rename(columns={\"asin\":\"cleaned_total_loreal_product\"})\r\n",
        "\r\n",
        "# final_products_df = pd.merge(total_products_df,loreal_products_df,left_on=['acc_num'],right_on=['acc_num'],how='left')\r\n",
        "# profiles_df = pd.merge(profiles_df,final_products_df,left_on=['acc_num'],right_on=['acc_num'],how='left')\r\n",
        "# for column in ['cleaned_total_product','cleaned_total_loreal_product']:\r\n",
        "#     profiles_df[column] = profiles_df[column].fillna(value=0)\r\n",
        "# profiles_df['cleaned_proportion_loreal_product'] = profiles_df['cleaned_total_loreal_product'] / profiles_df['cleaned_total_product']\r\n",
        "# profiles_df['cleaned_proportion_loreal_product'] = profiles_df['cleaned_proportion_loreal_product'].fillna(value=0)\r\n",
        "\r\n",
        "# display(profiles_df)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1K7W8TS2Qb"
      },
      "source": [
        "### Badges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LNX07UcS3Mc"
      },
      "source": [
        "# display(profiles_df[profiles_df['badges'].notnull()])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApTFPNMDTkDE"
      },
      "source": [
        "### Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Heh85tT_Tk5s"
      },
      "source": [
        "# profiles_df['cleaned_ranking'] = profiles_df['ranking'].fillna(value=0)\n",
        "# profiles_df['cleaned_ranking'] = profiles_df['cleaned_ranking'].astype(str).apply(lambda x: x.replace(',','')).astype(int)\n",
        "\n",
        "# display(profiles_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_97xzPK4wB1I"
      },
      "source": [
        "### Replace missing Rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY7ZIsuzwD7C"
      },
      "source": [
        "# products_df = pd.read_csv('consolidated_product_info.csv',encoding='utf-8')\n",
        "reviews_df = pd.read_csv('consolidated_products_feature_engineering.csv',encoding='utf-8')\n",
        "\n",
        "# products_without_rating = list(set(products_df[products_df['cleaned_rating'].isnull()]['asin']))\n",
        "# average_rating_df = reviews_df[reviews_df['ASIN'].isin(products_without_rating)].groupby('ASIN').agg({\"cleaned_ratings\": np.mean}).reset_index()\n",
        "\n",
        "# temp_df = products_df[['asin','cleaned_rating']].set_index(\"asin\").cleaned_rating.fillna(average_rating_df.set_index(\"ASIN\").cleaned_ratings).reset_index()\n",
        "# temp_df['cleaned_rating'] = temp_df['cleaned_rating'].fillna(value=0)\n",
        "\n",
        "# products_df = products_df.drop(columns=['cleaned_rating'])\n",
        "# products_df = pd.merge(products_df,temp_df,left_on=['asin'], right_on = ['asin'], how = 'left')\n",
        "# products_df['cleaned_rating'] = products_df['cleaned_rating'].fillna(value=0)\n",
        "# display(products_df)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytsd16FZ2be"
      },
      "source": [
        "### Engineer Total & Proportion of suspicious reviewers and brand repeats per product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nN1QsHPZ6p6"
      },
      "source": [
        "\r\n",
        "# # total_users_posted_df = reviews_df.groupby('ASIN').size().reset_index(name='cleaned_total_users_posted')\r\n",
        "# total_users_posted_df = products_df[['asin','clean_reviews_posted']]\r\n",
        "\r\n",
        "# interested_columns = ['cleaned_single_day_reviewer','cleaned_never_verified_reviewer','cleaned_one_hit_wonder','cleaned_take_back_reviewer','cleaned_brand_monogamist','cleaned_brand_loyalist','cleaned_brand_repeater']\r\n",
        "# temp_df = pd.merge(reviews_df,profiles_df[['acc_num'] + interested_columns],left_on=['acc_num'], right_on = ['acc_num'], how = 'left')\r\n",
        "\r\n",
        "# new_columns = []\r\n",
        "# for column in interested_columns:\r\n",
        "#     column_name_list = column.split(\"_\")\r\n",
        "#     total_column_name = column_name_list[0] + \"_total_\" + '_'.join(column_name_list[1:])\r\n",
        "#     proportion_column_name = column_name_list[0] + \"_proportion_\" + '_'.join(column_name_list[1:])\r\n",
        "#     new_columns += [total_column_name, proportion_column_name]\r\n",
        "\r\n",
        "#     current_df = temp_df[temp_df[column] == 1]\r\n",
        "#     current_df = current_df.groupby(['ASIN',column]).size().reset_index(name=total_column_name)\r\n",
        "\r\n",
        "#     current_df = pd.merge(current_df,total_users_posted_df,left_on=['ASIN'], right_on = ['asin'], how = 'left')\r\n",
        "#     current_df[proportion_column_name] = current_df[total_column_name] / current_df['clean_reviews_posted']\r\n",
        "#     total_users_posted_df = pd.merge(total_users_posted_df,current_df[['asin',total_column_name,proportion_column_name]],left_on=['asin'], right_on = ['asin'], how = 'left')\r\n",
        "\r\n",
        "# for column in new_columns:\r\n",
        "#     total_users_posted_df[column] = total_users_posted_df[column].fillna(value=0)\r\n",
        "# display(total_users_posted_df)\r\n",
        "# # total_users_class_df = temp_df.groupby('ASIN').agg({\"cleaned_single_day_reviewer\":np.sum,'cleaned_never_verified_reviewer':np.sum,'cleaned_one_hit_wonder':np.sum,'cleaned_take_back_reviewer':np.sum,'cleaned_brand_monogamist':np.sum,'cleaned_brand_loyalist':np.sum,'cleaned_brand_repeater':np.sum}).reset_index()\r\n",
        "# # temp_df = pd.merge(total_users_posted_df,total_users_class_df,left_on=['ASIN'], right_on = ['ASIN'], how = 'left')\r\n",
        "# # for column in temp_df.columns:\r\n",
        "# #     if column not in ['ASIN','total_users_posted']:\r\n",
        "# #         column_name_list = column.split(\"_\")\r\n",
        "# #         new_column_name = column_name_list[0] + \"_proportion_\" + '_'.join(column_name_list[1:])\r\n",
        "# #         temp_df[new_column_name] = temp_df[column] / temp_df['total_users_posted']\r\n",
        "# # display(temp_df)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCpIP3l0SAM3"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcVo9iRLRzb4"
      },
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "# vectorizer = TfidfVectorizer (ngram_range = (1,2))\r\n",
        "# text = list(reviews_df['cleaned_text'].astype(str))\r\n",
        "\r\n",
        "# vectorizer.fit(text)\r\n",
        "\r\n",
        "# reviews_tfidf = vectorizer.transform(text)\r\n",
        "# print(\"Saving TFIDF Reviews Dataset...\")\r\n",
        "# reviews_tfidf_df = pd.DataFrame(reviews_tfidf.toarray(), columns=vectorizer.get_feature_names())\r\n",
        "# reviews_tfidf_df = reviews_tfidf_df.fillna(0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY5EMgA77Maa"
      },
      "source": [
        "### Get Glove Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGHxsK37QQ6"
      },
      "source": [
        "import tqdm\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from gensim.models.fasttext import FastText\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
        "from gensim.test.utils import get_tmpfile\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "def check_null_text(text):\r\n",
        "    try:\r\n",
        "        null_data = float(text)\r\n",
        "        if math.isnan(null_data):\r\n",
        "            return \"\"\r\n",
        "        else:\r\n",
        "            return str(text)\r\n",
        "    except:\r\n",
        "        return str(text)\r\n",
        "\r\n",
        "def temp_new_text(text):\r\n",
        "    new_text = []\r\n",
        "    for data in text:\r\n",
        "        new_text.append(check_null_text(data))\r\n",
        "    return new_text\r\n",
        "\r\n",
        "def create_embedding_df(reviews, model):\r\n",
        "    vec = CountVectorizer(ngram_range = (1,1))\r\n",
        "    vec.fit_transform(reviews)\r\n",
        "    embedding_df = pd.DataFrame(columns=vec.get_feature_names())\r\n",
        "\r\n",
        "    for i in tqdm.tqdm(range(len(reviews))):\r\n",
        "        words = reviews[i].split()\r\n",
        "        for word in words:\r\n",
        "            embedding_df.at[i, word] = get_vector_value(word, model)\r\n",
        "                \r\n",
        "    return embedding_df.fillna(value=0)\r\n",
        "\r\n",
        "def get_vector_value(word,model):\r\n",
        "    try:\r\n",
        "        vector_value = np.mean(model.wv[word])\r\n",
        "    except Exception as e:\r\n",
        "        vector_value = 0\r\n",
        "    return vector_value\r\n",
        "\r\n",
        "def get_glove_vector(reviews):\r\n",
        "        print(\"Converting .txt file into .word2vec...\")\r\n",
        "        glove2word2vec('glove.6B.300d.txt', 'glove.6B.300d.txt.word2vec')\r\n",
        "        \r\n",
        "        # load the Stanford GloVe model\r\n",
        "        print(\"Loading GloVe model...\")\r\n",
        "        glove_model = KeyedVectors.load_word2vec_format('glove.6B.300d.txt.word2vec', binary=False)\r\n",
        "        \r\n",
        "        print(\"Generating Vector with GloVe...\")\r\n",
        "        glove_reviews_df = create_embedding_df(reviews, glove_model)\r\n",
        "        \r\n",
        "        return glove_reviews_df.fillna(value=0)\r\n",
        "\r\n",
        "#glove_reviews_df = get_glove_vector(temp_new_text(reviews_df['cleaned_text'].astype(str)))\r\n",
        "#glove_reviews_df.to_csv('glove_embedding.csv',index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwMU4OAf_UHv"
      },
      "source": [
        "## Get fastText vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUDUFZGw_YtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2cdfc0-53a3-4bcd-ba2e-f946f51c092f"
      },
      "source": [
        "def get_fasttext_vector(reviews):\r\n",
        "    # parameters to train fast text\r\n",
        "    embedding_size = 300\r\n",
        "    window_size = 5\r\n",
        "    min_word = 5\r\n",
        "    down_sampling = 1e-2\r\n",
        "\r\n",
        "    # retrieve fasttext vector\r\n",
        "    print(\"Generating fastText model...\")\r\n",
        "    ft_model = FastText(reviews,\r\n",
        "                size=embedding_size,\r\n",
        "                window=window_size,\r\n",
        "                min_count=min_word,\r\n",
        "                sample=down_sampling,\r\n",
        "                sg=0, # put 1 if you want to use skip-gram, look into the documentation for other variables\r\n",
        "                iter=100)\r\n",
        "    ft_model.save(\"fasttext.model\")\r\n",
        "    # ft_model = FastText.load(get_tmpfile(self.config.fasttext.model_file)) # keep this code for future development\r\n",
        "\r\n",
        "    print(\"Generating Vector with fastText...\")\r\n",
        "    ft_reviews_df = create_embedding_df(reviews, ft_model)\r\n",
        "                \r\n",
        "    return ft_reviews_df.fillna(value=0)\r\n",
        "\r\n",
        "ft_reviews_df = get_fasttext_vector(temp_new_text(reviews_df['cleaned_text'].astype(str)))\r\n",
        "ft_reviews_df.to_csv('fasttext_embedding.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating fastText model...\n",
            "Generating Vector with fastText...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 5236/85126 [34:52<17:35:13,  1.26it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFjLD67O_p3c"
      },
      "source": [
        "## Get word2Vec vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVT0Al47_sHS"
      },
      "source": [
        "def get_word2vec_vector(reviews):\r\n",
        "    # parameters to train fast text\r\n",
        "    embedding_size = 300\r\n",
        "    window_size = 5\r\n",
        "    min_word = 5\r\n",
        "    down_sampling = 1e-2\r\n",
        "\r\n",
        "    # retrieve glove vector\r\n",
        "    print(\"Generating Word2Vec model...\")\r\n",
        "    word2vec_model = Word2Vec(reviews,\r\n",
        "                size=embedding_size,\r\n",
        "                window=window_size,\r\n",
        "                min_count=min_word,\r\n",
        "                sample=down_sampling,\r\n",
        "                sg=0, # put 1 if you want to use skip-gram, look into the documentation for other variables\r\n",
        "                iter=100)\r\n",
        "    word2vec_model.save('word2vec.model')\r\n",
        "\r\n",
        "    # word2vec_model = FastText.load(get_tmpfile(self.config.word2vec.model_file)) # keeping this code for future references\r\n",
        "\r\n",
        "    print(\"Generating Vector with Word2Vec...\")\r\n",
        "    word2vec_reviews_df = create_embedding_df(reviews, word2vec_model)\r\n",
        "                \r\n",
        "    return word2vec_reviews_df.fillna(value=0)\r\n",
        "\r\n",
        "word2vec_reviews_df = get_word2vec_vector(temp_new_text(reviews_df['cleaned_text'].astype(str)))\r\n",
        "word2vec_reviews_df.to_csv('word2vec_embedding.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTENiCM0c5Ex"
      },
      "source": [
        "### Tuning DBScan Hyperparam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DovEyyDec4vq"
      },
      "source": [
        "# cd_drive('processed')\r\n",
        "# fake_features_framework_df = pd.read_csv('fake_framework_features.csv',encoding='utf-8')\r\n",
        "# modelling_df = fake_features_framework_df.drop(columns=['asin','acc_num','cleaned_reviews_profile_link','cleaned_reviews_text'])\r\n",
        "# modelling_df = modelling_df.fillna(value=0)\r\n",
        "# display(modelling_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7LKPy1VW3S_"
      },
      "source": [
        "# from sklearn.metrics import silhouette_samples,silhouette_score\r\n",
        "# from sklearn.cluster import KMeans\r\n",
        "\r\n",
        "# range_n_clusters = range(3,10)\r\n",
        "# kmeans_silhouette_results = {}\r\n",
        "# test_df = modelling_df.sample(100)\r\n",
        "\r\n",
        "# for n_clusters in range_n_clusters:\r\n",
        "#     clusterer = KMeans(n_clusters=n_clusters, random_state = 42)\r\n",
        "#     cluster_labels = clusterer.fit_predict(test_df)\r\n",
        "\r\n",
        "#     silhouette_avg = silhouette_score(test_df, cluster_labels)\r\n",
        "#     kmeans_silhouette_results[n_clusters] = silhouette_avg\r\n",
        "#     print(\"For n_clusters =\",n_clusters,\"The average silhouette_score is :\", silhouette_avg)\r\n",
        "\r\n",
        "# chosen_cluster = max(kmeans_silhouette_results, key=kmeans_silhouette_results.get)\r\n",
        "# chosen_cluster_silhouette_score = kmeans_silhouette_results[chosen_cluster]\r\n",
        "# print(\"\\nChosen cluster by K-Means is {}. Its Silhouette score is {}\".format(chosen_cluster,chosen_cluster_silhouette_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHA8GXdLdkgJ"
      },
      "source": [
        "# from sklearn.cluster import DBSCAN\r\n",
        "\r\n",
        "# eps_silhouette_results = {}\r\n",
        "\r\n",
        "# eps = 25\r\n",
        "# current_silhouette = 0\r\n",
        "# kmeans_silhouette = chosen_cluster_silhouette_score\r\n",
        "# cluster = chosen_cluster\r\n",
        "# difference = 0.02\r\n",
        "\r\n",
        "# decay = 0.99\r\n",
        "# min_eps = 0.1\r\n",
        "\r\n",
        "# while difference > 0.01:\r\n",
        "#     dbscan_model = DBSCAN(eps=eps,min_samples=cluster).fit(test_df)\r\n",
        "#     core_samples_mask = np.zeros_like(dbscan_model.labels_,dtype=bool)\r\n",
        "#     core_samples_mask[dbscan_model.core_sample_indices_] = True\r\n",
        "#     labels = dbscan_model.labels_\r\n",
        "#     silhouette_avg = silhouette_score(test_df, labels)\r\n",
        "\r\n",
        "#     difference = round(abs(silhouette_avg - kmeans_silhouette),)\r\n",
        "#     current_silhouette = silhouette_avg\r\n",
        "#     eps_silhouette_results[eps] = silhouette_avg\r\n",
        "#     print(\"For eps value = {}, the average silhouete_score is : {}\".format(eps, silhouette_avg))\r\n",
        "#     eps = max(min_eps, eps*decay)\r\n",
        "\r\n",
        "# chosen_eps = max(eps_silhouette_results, key=eps_silhouette_results.get)\r\n",
        "# chosen_eps_silhouette_score = eps_silhouette_results[chosen_eps]\r\n",
        "# print(\"Chosen eps is {}. Its average silhouette score is {}\\n\".format(chosen_eps,chosen_eps_silhouette_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNzmcUTia6F"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# min_sample_results = {}\r\n",
        "# min_sample_value = 1\r\n",
        "# clusters = None\r\n",
        "# while clusters != chosen_cluster:\r\n",
        "#     dbscan_model = DBSCAN(eps=chosen_eps,min_samples=min_sample_value).fit(test_df)\r\n",
        "#     core_samples_mask = np.zeros_like(dbscan_model.labels_,dtype=bool)\r\n",
        "#     core_samples_mask[dbscan_model.core_sample_indices_] = True\r\n",
        "#     labels = set([label for label in dbscan_model.labels_ if label >=0])\r\n",
        "#     clusters = len(set(labels))\r\n",
        "#     print(\"For min_samples value =\",min_sample_value,\"Total no. of clusters are :\",clusters)\r\n",
        "#     min_sample_value += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uczXQZBTxO1y"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CbyjpaQxQbN"
      },
      "source": [
        "# cd_drive('processed')\r\n",
        "# fake_features_framework_df = pd.read_csv('fake_framework_features.csv',encoding='utf-8')\r\n",
        "# modelling_df = fake_features_framework_df.drop(columns=['asin','acc_num','cleaned_reviews_profile_link','cleaned_reviews_text'])\r\n",
        "# modelling_df = modelling_df.fillna(value=0)\r\n",
        "\r\n",
        "# from sklearn.feature_selection import VarianceThreshold\r\n",
        "\r\n",
        "# total_features = len(modelling_df.columns)\r\n",
        "# v_threshold = VarianceThreshold(threshold=0)\r\n",
        "# v_threshold.fit(modelling_df)\r\n",
        "# importance_by_variance = v_threshold.get_support()\r\n",
        "# importance_by_variance_dict = dict(zip(modelling_df.columns,importance_by_variance))\r\n",
        "# unimportant_features = list(importance_by_variance).count(False)\r\n",
        "# print(\"Total Features: {}\\nNumber of unimportant features: {}\\nConsists of {}% of the features in dataset based on variance.\\nDropping...Proceeding with Feature Selection using Dispersion Ratio...\".format(total_features, unimportant_features,(unimportant_features/total_features)*100))\r\n",
        "\r\n",
        "# current_features = []\r\n",
        "# for key,value in importance_dict.items():\r\n",
        "#     if value == True:\r\n",
        "#         current_features.append(key)\r\n",
        "\r\n",
        "# modelling_df = modelling_df[current_features]\r\n",
        "# import math\r\n",
        "\r\n",
        "# def dispersion(data):\r\n",
        "#     current_features = []\r\n",
        "#     data = data +1 #avoid 0 division\r\n",
        "#     aritmeticMean = np.mean(data, axis =0 )\r\n",
        "#     geometricMean = np.power(np.prod(data, axis =0 ),1/data.shape[0])\r\n",
        "#     R = aritmeticMean/geometricMean\r\n",
        "#     for i in range(len(R)):\r\n",
        "#         dispersion_value = R[i]\r\n",
        "#         if dispersion_value >= 1:\r\n",
        "#             current_features.append(data.columns[i])\r\n",
        "#     return current_features\r\n",
        "\r\n",
        "# current_features = dispersion(modelling_df)\r\n",
        "\r\n",
        "# print(\"Number of important features by dispersion ratio: {}\".format(len(current_features)))\r\n",
        "# print(\"Final Features are\", ', '.join(current_features)[:-2])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}